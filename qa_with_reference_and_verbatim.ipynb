{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4993da90",
   "metadata": {},
   "source": [
    "# QA with reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf56f04",
   "metadata": {},
   "source": [
    "We believe that hallucinations pose a major problem in the adoption of LLMs (Language Model Models). It is imperative to provide a simple and quick solution that allows the user to verify the coherence of the answers to the questions they are asked.\n",
    "\n",
    "The conventional approach is to provide a list of URLs of the documents that helped in answering (see qa_with_source). However, this approach is unsatisfactory in several scenarios:\n",
    "\n",
    "1. The question is asked about a PDF of over 100 pages. Each fragment comes from the same document, but from where?\n",
    "2. Some documents do not have URLs (data retrieved from a database or other loaders).\n",
    "\n",
    "It appears essential to have a means of retrieving all references to the actual data sources used by the model to answer the question. \n",
    "\n",
    "This includes:\n",
    "- The precise list of documents used for the answer (the `Documents`, along with their metadata that may contain page numbers, slide numbers, or any other information allowing the retrieval of the fragment in the original document).\n",
    "- The excerpts of text used for the answer in each fragment. Even if a fragment is used, the LLM only utilizes a small portion to generate the answer. Access to these verbatim excerpts helps to quickly ascertain the validity of the answer.\n",
    "\n",
    "We propose a new pipeline: `qa_with_reference` for this purpose. It is a Question/Answer type pipeline that returns the list of documents used, and in the metadata, the list of verbatim excerpts exploited to produce the answer.\n",
    "\n",
    "*At this time, only the `map_reduce` chain type car extract the verbatim excerpts.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b0fd3-82d3-439a-87b8-3ef77d9d1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'langchain-qa_with_references' openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d181ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.schema import Document\n",
    "llm = OpenAI(\n",
    "            max_tokens=1000,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d935867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer \"He eats apples, pears and carrots.\", the LLM use:\n",
      "Document 0\n",
      "- \"he eats apples\"\n",
      "- \"he eats pears.\"\n",
      "Document 1\n",
      "- \"he eats carrots.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_qa_with_references.chains import QAWithReferencesAndVerbatimsChain\n",
    "chain_type=\"map_reduce\"  # Only map_reduce can extract the verbatim.\n",
    "qa_chain = QAWithReferencesAndVerbatimsChain.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=chain_type,\n",
    "    )\n",
    "\n",
    "question = \"what does it eat?\"\n",
    "bodies = [\"he eats apples and plays football.\"\n",
    "          \"My name is Philippe.\"\n",
    "          \n",
    "          \"he eats pears.\",\n",
    "          \n",
    "          \"he eats carrots. I like football.\",\n",
    "          \"The Earth is round.\"\n",
    "]\n",
    "docs=[Document(page_content=body,metadata={\"id\":i}) for i,body in enumerate(bodies)]\n",
    "\n",
    "answer = qa_chain(\n",
    "        inputs={\n",
    "            \"docs\": docs,\n",
    "            \"question\": question,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "print(f'To answer \"{answer[\"answer\"]}\", the LLM use:')\n",
    "for doc in answer[\"source_documents\"]:\n",
    "    print(f\"Document {doc.metadata['id']}\")\n",
    "    for verbatim in doc.metadata.get(\"verbatims\",[]):\n",
    "        print(f'- \"{verbatim}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbee746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296c49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "question = \"what is the Machine learning?\"\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever()\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "docs = wikipedia_retriever.get_relevant_documents(question)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "split_docs = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=10\n",
    ").split_documents(docs)\n",
    "\n",
    "vectorstore.add_documents(split_docs)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e9e8e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the question \"what is the Machine learning?\", to answer \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms. As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed 'neural networks'; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Machine Learning is a field of study that uses mathematical optimization (mathematical programming) methods to develop algorithms for tasks such as language models, computer vision, speech recognition, email filtering, agriculture and medicine. The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period. By the early 1960s an experimental 'learning machine' with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning.\", the LLM use:\n",
      "Source https://en.wikipedia.org/wiki/Machine_learning\n",
      "-  \"The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_qa_with_references.chains import RetrievalQAWithReferencesAndVerbatimsChain\n",
    "from typing import Literal\n",
    "chain_type:Literal[\"stuff\",\"map_reduce\",\"map_rerank\",\"refine\"]=\"map_reduce\"\n",
    "\n",
    "qa_chain = RetrievalQAWithReferencesAndVerbatimsChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n",
    "answer = qa_chain(\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "    }\n",
    ")\n",
    "\n",
    "def merge_result_by_urls(answer):\n",
    "    references = {}\n",
    "    for doc in answer[\"source_documents\"]:\n",
    "        source = doc.metadata.get(\"source\", [])\n",
    "        verbatims_for_source: List[str] = doc.metadata.get(source, [])\n",
    "        verbatims_for_source.extend(doc.metadata.get(\"verbatims\", []))\n",
    "        references[source] = verbatims_for_source\n",
    "    return references\n",
    "\n",
    "print(f'For the question \"{question}\", to answer \"{answer[\"answer\"]}\", the LLM use:')\n",
    "references = merge_result_by_urls(answer)\n",
    "# Print the result\n",
    "for source, verbatims in references.items():\n",
    "    print(f\"Source {source}\")\n",
    "    for verbatim in verbatims:\n",
    "        print(f'-  \"{verbatim}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714b9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
