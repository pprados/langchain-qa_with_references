{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7abf8fc",
   "metadata": {},
   "source": [
    "# QA with reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f538a0",
   "metadata": {},
   "source": [
    "We believe that hallucinations pose a major problem in the adoption of LLMs (Language Model Models). It is imperative to provide a simple and quick solution that allows the user to verify the coherence of the answers to the questions they are asked.\n",
    "\n",
    "The conventional approach is to provide a list of URLs of the documents that helped in answering (see qa_with_source). However, this approach is unsatisfactory in several scenarios:\n",
    "\n",
    "1. The question is asked about a PDF of over 100 pages. Each fragment comes from the same document, but from where?\n",
    "2. Some documents do not have URLs (data retrieved from a database or other loaders).\n",
    "\n",
    "It appears essential to have a means of retrieving all references to the actual data sources used by the model to answer the question. \n",
    "\n",
    "This includes:\n",
    "- The precise list of documents used for the answer (the `Documents`, along with their metadata that may contain page numbers, slide numbers, or any other information allowing the retrieval of the fragment in the original document).\n",
    "- The excerpts of text used for the answer in each fragment. Even if a fragment is used, the LLM only utilizes a small portion to generate the answer. Access to these verbatim excerpts helps to quickly ascertain the validity of the answer.\n",
    "\n",
    "We propose a new pipeline: `qa_with_reference` for this purpose. It is a Question/Answer type pipeline that returns the list of documents used, and in the metadata, the list of verbatim excerpts exploited to produce the answer.\n",
    "\n",
    "*At this time, only the `map_reduce` chain type car extract the verbatim excerpts.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e49893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-qa_with_references[openai] in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (0.0.285)\n",
      "Requirement already satisfied: langchain>=0.0.285 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain-qa_with_references[openai]) (0.0.285)\n",
      "Collecting openai<0.29,>=0.28 (from langchain-qa_with_references[openai])\n",
      "  Obtaining dependency information for openai<0.29,>=0.28 from https://files.pythonhosted.org/packages/ae/59/911d6e5f1d7514d79c527067643376cddcf4cb8d1728e599b3b03ab51c69/openai-0.28.0-py3-none-any.whl.metadata\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken<0.4.0,>=0.3.2 (from langchain-qa_with_references[openai])\n",
      "  Using cached tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (0.5.9)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (0.0.35)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from langchain>=0.0.285->langchain-qa_with_references[openai]) (8.2.3)\n",
      "Requirement already satisfied: tqdm in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from openai<0.29,>=0.28->langchain-qa_with_references[openai]) (4.66.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from tiktoken<0.4.0,>=0.3.2->langchain-qa_with_references[openai]) (2023.8.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.285->langchain-qa_with_references[openai]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.285->langchain-qa_with_references[openai]) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.285->langchain-qa_with_references[openai]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.285->langchain-qa_with_references[openai]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.285->langchain-qa_with_references[openai]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.285->langchain-qa_with_references[openai]) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.285->langchain-qa_with_references[openai]) (3.20.1)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.285->langchain-qa_with_references[openai]) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.285->langchain-qa_with_references[openai]) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.0.285->langchain-qa_with_references[openai]) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.285->langchain-qa_with_references[openai]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.285->langchain-qa_with_references[openai]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.285->langchain-qa_with_references[openai]) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.285->langchain-qa_with_references[openai]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.285->langchain-qa_with_references[openai]) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.285->langchain-qa_with_references[openai]) (1.0.0)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: tiktoken, openai\n",
      "Successfully installed openai-0.28.0 tiktoken-0.3.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install 'langchain-qa_with_references[openai]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51149c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.schema import Document\n",
    "llm = OpenAI(\n",
    "            temperature=0,\n",
    "            max_tokens=1000,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de242e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer \"He eats apples, pears and carrots.\", the LLM use:\n",
      "Document 0\n",
      "- \"he eats apples\"\n",
      "- \"he eats pears.\"\n",
      "Document 1\n",
      "- \"he eats carrots.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_qa_with_references.chains import QAWithReferencesAndVerbatimsChain\n",
    "chain_type=\"map_reduce\"  # Only map_reduce can extract the verbatim.\n",
    "qa_chain = QAWithReferencesAndVerbatimsChain.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=chain_type,\n",
    "    )\n",
    "\n",
    "question = \"what does it eat?\"\n",
    "bodies = [\"he eats apples and plays football.\"\n",
    "          \"My name is Philippe.\"\n",
    "          \n",
    "          \"he eats pears.\",\n",
    "          \n",
    "          \"he eats carrots. I like football.\",\n",
    "          \"The Earth is round.\"\n",
    "]\n",
    "docs=[Document(page_content=body,metadata={\"id\":i}) for i,body in enumerate(bodies)]\n",
    "\n",
    "answer = qa_chain(\n",
    "        inputs={\n",
    "            \"docs\": docs,\n",
    "            \"question\": question,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "print(f'To answer \"{answer[\"answer\"]}\", the LLM use:')\n",
    "for doc in answer[\"source_documents\"]:\n",
    "    print(f\"Document {doc.metadata['id']}\")\n",
    "    for verbatim in doc.metadata.get(\"verbatims\",[]):\n",
    "        print(f'- \"{verbatim}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be14e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chroma in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: wikipedia in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/pprados/workspace.bda/cpackage/venv/langchain-qa-with-references-VmF69Zxq-py3.10/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install chroma wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4efd1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever()\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"/tmp/chroma_db_oai\",\n",
    ")\n",
    "docs = wikipedia_retriever.get_relevant_documents(question)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "split_docs = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=10\n",
    ").split_documents(docs)\n",
    "\n",
    "vectorstore.add_documents(split_docs)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "question = \"what is the Machine learning?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b4861ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the question \"what is the Machine learning?\", to answer \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines 'discover' their 'own' algorithms, without needing to be explicitly told what to do by any human-developed algorithms.\", the LLM use:\n",
      "Source https://en.wikipedia.org/wiki/Machine_learning\n",
      "-  \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines \"discover\" their \"own\" algorithms, without needing to be explicitly told what to do by any human-developed algorithms.\"\n",
      "Source https://en.wikipedia.org/wiki/Machine_learning\n",
      "-  \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines \"discover\" their \"own\" algorithms, without needing to be explicitly told what to do by any human-developed algorithms.\"\n",
      "Source https://en.wikipedia.org/wiki/Machine_learning\n",
      "-  \"Machine learning (ML) is an umbrella term for solving problems for which development of algorithms by human programmers would be cost-prohibitive, and instead the problems are solved by helping machines \"discover\" their \"own\" algorithms, without needing to be explicitly told what to do by any human-developed algorithms.\"\n",
      "Source https://en.wikipedia.org/wiki/Machine_learning\n",
      "-  \"machine learning grew out of the quest for artificial intelligence (AI\"\n",
      "-  \"attempted to approach the problem with various symbolic methods\"\n",
      "-  \"neural networks\"\n",
      "-  \"perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_qa_with_references.chains import RetrievalQAWithReferencesAndVerbatimsChain\n",
    "from typing import Literal\n",
    "chain_type:Literal[\"stuff\",\"map_reduce\",\"map_rerank\",\"refine\"]=\"map_reduce\"\n",
    "\n",
    "qa_chain = RetrievalQAWithReferencesAndVerbatimsChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n",
    "answer = qa_chain(\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "    }\n",
    ")\n",
    "print(f'For the question \"{question}\", to answer \"{answer[\"answer\"]}\", the LLM use:')\n",
    "for doc in answer[\"source_documents\"]:\n",
    "    print(f\"Source {doc.metadata['source']}\")\n",
    "    for verbatim in doc.metadata.get(\"verbatims\", []):\n",
    "        print(f'-  \"{verbatim}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464afff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_with_ref",
   "language": "python",
   "name": "qa_with_ref"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
